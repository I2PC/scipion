# -*- conf -*-

[localhost]
PARALLEL_COMMAND = mpirun -np %_(JOB_NODES)d -bynode %_(COMMAND)s
NAME = PBS/TORQUE
MANDATORY = False
SUBMIT_COMMAND = qsub %_(JOB_SCRIPT)s
SUBMIT_TEMPLATE = #!/bin/bash
    ### Inherit all current environment variables
    #PBS -V
    ### Job name
    #PBS -N %_(JOB_NAME)s
    ### Queue name
    ###PBS -q %_(JOB_QUEUE)s
    ### Standard output and standard error messages
    #PBS -k eo
    ### Specify the number of nodes and thread (ppn) for your job.
    #PBS -l nodes=%_(JOB_NODES)d:ppn=%_(JOB_THREADS)d
    ### Tell PBS the anticipated run-time for your job, where walltime=HH:MM:SS
    #PBS -l walltime=%_(JOB_HOURS)d:00:00
    # Use as working dir the path where qsub was launched
    WORKDIR=$PBS_O_WORKDIR
    #################################
    ### Set environment varible to know running mode is non interactive
    export XMIPP_IN_QUEUE=1
    ### Switch to the working directory;
    cd $WORKDIR
    # Make a copy of PBS_NODEFILE
    cp $PBS_NODEFILE %_(JOB_NODEFILE)s
    # Calculate the number of processors allocated to this run.
    NPROCS=`wc -l < $PBS_NODEFILE`
    # Calculate the number of nodes allocated.
    NNODES=`uniq $PBS_NODEFILE | wc -l`
    ### Display the job context
    echo Running on host `hostname`
    echo Time is `date`
    echo Working directory is `pwd`
    echo Using ${NPROCS} processors across ${NNODES} nodes
    echo PBS_NODEFILE:
    cat $PBS_NODEFILE
    #################################
    %_(JOB_COMMAND)s
CANCEL_COMMAND = canceljob %_(JOB_ID)s
CHECK_COMMAND = qstat %_(JOB_ID)s
QUEUES = { "default": {} }
